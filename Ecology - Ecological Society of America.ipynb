{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, json\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "from bs4.diagnose import diagnose\n",
    "from parsel import Selector\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "ecology = driver.get(\"https://esajournals.onlinelibrary.wiley.com/loi/19399170\")\n",
    "ecology_content = driver.page_source\n",
    "issue_archive = SoupStrainer(class_='rlist loi__list')\n",
    "browse_soup = BeautifulSoup(ecology_content, 'html.parser', parse_only = issue_archive)\n",
    "\n",
    "volume_links = []\n",
    "\n",
    "for vol_link in browse_soup.find_all('a'):\n",
    "    volume_links.append(vol_link.get('href'))\n",
    "    \n",
    "##After getting all the volume links we acces them\n",
    "\n",
    "issue_links = []\n",
    "for volume in volume_links:\n",
    "    eco_volume = driver.get(\"https://esajournals.onlinelibrary.wiley.com\" + volume)\n",
    "    eco_volume_content = driver.page_source\n",
    "    volume_issues = SoupStrainer(class_='rlist loi__issues')\n",
    "    browse_volume_issues = BeautifulSoup(eco_volume_content, 'html.parser', parse_only = volume_issues)\n",
    "    \n",
    "    for issue_link in browse_volume_issues.find_all('a'):\n",
    "        issue_links.append(issue_link.get('href'))\n",
    "\n",
    "##After getting all the issue links in the volume, access and extract titles and related links\n",
    "titles = []\n",
    "doi = []\n",
    "for issue in issue_links:\n",
    "    volume_issue = driver.get(\"https://esajournals.onlinelibrary.wiley.com\" + issue)\n",
    "    volume_issue_content = driver.page_source\n",
    "    articles = SoupStrainer(class_='issue-item__title visitable')\n",
    "    browse_articles = BeautifulSoup(volume_issue_content, 'html.parser', parse_only = articles)\n",
    "    \n",
    "    for title in browse_articles.find_all('h2'):\n",
    "        titles.append(title.get_text())\n",
    "        \n",
    "    for title_doi in browse_articles.find_all('a'):\n",
    "        doi.append(title_doi.get('href'))\n",
    "\n",
    "\n",
    "article_dict = dict(zip(titles, doi))\n",
    "with open('Ecology.json', 'w+') as fp:\n",
    "    json.dump(article_dict, fp,indent = \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
